import datetime
from typing import TYPE_CHECKING, Any, Callable, Iterator, Literal, Optional, Union, overload

import boto3
import pandas as pd
import pyarrow as pa

from awswrangler.typing import RayReadParquetSettings

if TYPE_CHECKING:
    from mypy_boto3_s3 import S3Client

def _pyarrow_parquet_file_wrapper(
    source: Any, coerce_int96_timestamp_unit: Optional[str] = ...
) -> pa.parquet.ParquetFile: ...
def _read_parquet_metadata_file(
    s3_client: Optional["S3Client"],
    path: str,
    s3_additional_kwargs: Optional[dict[str, str]],
    use_threads: Union[bool, int],
    version_id: Optional[str] = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
) -> pa.schema: ...
def _read_parquet(  # pylint: disable=W0613
    paths: list[str],
    path_root: Optional[str],
    schema: Optional[pa.schema],
    columns: Optional[list[str]],
    coerce_int96_timestamp_unit: Optional[str],
    use_threads: Union[bool, int],
    parallelism: int,
    version_ids: Optional[dict[str, str]],
    s3_client: Optional["S3Client"],
    s3_additional_kwargs: Optional[dict[str, Any]],
    arrow_kwargs: dict[str, Any],
    bulk_read: bool,
) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]: ...
def _read_parquet_metadata(
    path: Union[str, list[str]],
    path_suffix: Optional[str],
    path_ignore_suffix: Union[str, list[str], None],
    ignore_empty: bool,
    ignore_null: bool,
    dtype: Optional[dict[str, str]],
    sampling: float,
    dataset: bool,
    use_threads: Union[bool, int],
    boto3_session: Optional[boto3.Session],
    s3_additional_kwargs: Optional[dict[str, str]],
    version_id: Optional[Union[str, dict[str, str]]] = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
) -> tuple[dict[str, str], Optional[dict[str, str]], Optional[dict[str, list[str]]]]: ...
def read_parquet_metadata(
    path: Union[str, list[str]],
    dataset: bool = ...,
    version_id: Optional[Union[str, dict[str, str]]] = ...,
    path_suffix: Optional[str] = ...,
    path_ignore_suffix: Union[str, list[str], None] = ...,
    ignore_empty: bool = ...,
    ignore_null: bool = ...,
    dtype: Optional[dict[str, str]] = ...,
    sampling: float = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    use_threads: Union[bool, int] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> tuple[dict[str, str], Optional[dict[str, str]]]: ...
@overload
def read_parquet(
    path: Union[str, list[str]],
    path_root: Optional[str] = ...,
    dataset: bool = ...,
    path_suffix: Union[str, list[str], None] = ...,
    path_ignore_suffix: Union[str, list[str], None] = ...,
    ignore_empty: bool = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    schema: Optional[pa.Schema] = ...,
    last_modified_begin: Optional[datetime.datetime] = ...,
    last_modified_end: Optional[datetime.datetime] = ...,
    version_id: Optional[Union[str, dict[str, str]]] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: Literal[False] = ...,
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> pd.DataFrame: ...
@overload
def read_parquet(
    path: Union[str, list[str]],
    *,
    path_root: Optional[str] = ...,
    dataset: bool = ...,
    path_suffix: Union[str, list[str], None] = ...,
    path_ignore_suffix: Union[str, list[str], None] = ...,
    ignore_empty: bool = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    schema: Optional[pa.Schema] = ...,
    last_modified_begin: Optional[datetime.datetime] = ...,
    last_modified_end: Optional[datetime.datetime] = ...,
    version_id: Optional[Union[str, dict[str, str]]] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: Literal[True],
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> Iterator[pd.DataFrame]: ...
@overload
def read_parquet(
    path: Union[str, list[str]],
    *,
    path_root: Optional[str] = ...,
    dataset: bool = ...,
    path_suffix: Union[str, list[str], None] = ...,
    path_ignore_suffix: Union[str, list[str], None] = ...,
    ignore_empty: bool = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    schema: Optional[pa.Schema] = ...,
    last_modified_begin: Optional[datetime.datetime] = ...,
    last_modified_end: Optional[datetime.datetime] = ...,
    version_id: Optional[Union[str, dict[str, str]]] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: bool,
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]: ...
@overload
def read_parquet(
    path: Union[str, list[str]],
    *,
    path_root: Optional[str] = ...,
    dataset: bool = ...,
    path_suffix: Union[str, list[str], None] = ...,
    path_ignore_suffix: Union[str, list[str], None] = ...,
    ignore_empty: bool = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    schema: Optional[pa.Schema] = ...,
    last_modified_begin: Optional[datetime.datetime] = ...,
    last_modified_end: Optional[datetime.datetime] = ...,
    version_id: Optional[Union[str, dict[str, str]]] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: int,
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> Iterator[pd.DataFrame]: ...
@overload
def read_parquet_table(
    table: str,
    database: str,
    *,
    filename_suffix: Union[str, list[str], None] = ...,
    filename_ignore_suffix: Union[str, list[str], None] = ...,
    catalog_id: Optional[str] = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: Literal[False] = ...,
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> pd.DataFrame: ...
@overload
def read_parquet_table(
    table: str,
    database: str,
    *,
    filename_suffix: Union[str, list[str], None] = ...,
    filename_ignore_suffix: Union[str, list[str], None] = ...,
    catalog_id: Optional[str] = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: Literal[True],
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> Iterator[pd.DataFrame]: ...
@overload
def read_parquet_table(
    table: str,
    database: str,
    *,
    filename_suffix: Union[str, list[str], None] = ...,
    filename_ignore_suffix: Union[str, list[str], None] = ...,
    catalog_id: Optional[str] = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: bool,
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]: ...
@overload
def read_parquet_table(
    table: str,
    database: str,
    *,
    filename_suffix: Union[str, list[str], None] = ...,
    filename_ignore_suffix: Union[str, list[str], None] = ...,
    catalog_id: Optional[str] = ...,
    partition_filter: Optional[Callable[[dict[str, str]], bool]] = ...,
    columns: Optional[list[str]] = ...,
    validate_schema: bool = ...,
    coerce_int96_timestamp_unit: Optional[str] = ...,
    dtype_backend: Literal["numpy_nullable", "pyarrow"] = ...,
    chunked: int,
    use_threads: Union[bool, int] = ...,
    ray_args: Optional[RayReadParquetSettings] = ...,
    boto3_session: Optional[boto3.Session] = ...,
    s3_additional_kwargs: Optional[dict[str, Any]] = ...,
    pyarrow_additional_kwargs: Optional[dict[str, Any]] = ...,
) -> Iterator[pd.DataFrame]: ...
